{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3ee0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5379199",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f50141ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10000\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "target_texts_input = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b03f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "\n",
    "with open(\"../neural_machine_translation/data/hin.txt\") as f:\n",
    "    for line in f:\n",
    "        \n",
    "        # keeping only limited number of samples\n",
    "        t += 1\n",
    "        if t > NUM_SAMPLES:\n",
    "            break\n",
    "            \n",
    "        if '\\t' not in line:\n",
    "            continue\n",
    "            \n",
    "        input_text, translation, *rest = line.rstrip().split('\\t')\n",
    "        \n",
    "        target_text = translation + ' <eos>'\n",
    "        target_text_input = '<sos> ' + translation\n",
    "        \n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "        target_texts_input.append(target_text_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b132a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> वाह!',\n",
       " '<sos> झुको!',\n",
       " '<sos> बतख़!',\n",
       " '<sos> बचाओ!',\n",
       " '<sos> उछलो.',\n",
       " '<sos> कूदो.',\n",
       " '<sos> छलांग.',\n",
       " '<sos> नमस्ते।',\n",
       " '<sos> नमस्कार।',\n",
       " '<sos> वाह-वाह!',\n",
       " '<sos> चियर्स!',\n",
       " '<sos> सांस छोड़।',\n",
       " '<sos> सांस छोड़ो।',\n",
       " '<sos> समझे कि नहीं?',\n",
       " '<sos> मैं ठीक हूँ।']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts_input[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fff2d5",
   "metadata": {},
   "source": [
    "### Tokenizers and word to index mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce426b5",
   "metadata": {},
   "source": [
    "#### I have two languages to deal with, hence need two different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "223c4b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow!', 'Duck!', 'Duck!', 'Help!', 'Jump.']\n"
     ]
    }
   ],
   "source": [
    "print(input_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a12a2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 16:07:55.786960: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from itertools import islice\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3152349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1326], [949], [949], [83], [582], [582], [582]]\n",
      "Found 2463 unique input tokens\n",
      "Maximum length of input sequences: 22\n"
     ]
    }
   ],
   "source": [
    "# tokenizer for inputs\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "print(input_sequences[:7])\n",
    "\n",
    "\n",
    "# word_to_index mapping \n",
    "word_to_idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens' % len(word_to_idx_inputs))\n",
    "first_10_mapping_inputs = dict(islice(word_to_idx_inputs.items(), 10))\n",
    "first_10_mapping_inputs\n",
    "\n",
    "\n",
    "# maximum length of input sequences\n",
    "max_len_inputs = max(len(i) for i in input_sequences)\n",
    "print(\"Maximum length of input sequences:\",max_len_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6fce566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1538, 1], [1539, 1], [1540, 1], [1541, 1], [1542, 1]]\n",
      "[[2, 1538], [2, 1539], [2, 1540], [2, 1541], [2, 1542]]\n",
      "Found 3265 unique output tokens\n",
      "Maximum length of output sequences: 26\n"
     ]
    }
   ],
   "source": [
    "# tokenizer for outputs\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_input)\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_input)\n",
    "print(target_sequences[:5])\n",
    "print(target_sequences_inputs[:5])\n",
    "\n",
    "\n",
    "# word to index mapping\n",
    "word_to_idx_outputs = tokenizer_outputs.word_index\n",
    "print(\"Found %s unique output tokens\" % len(word_to_idx_outputs))\n",
    "first_10_mappings_outputs = dict(islice(word_to_idx_outputs.items(), 10))\n",
    "first_10_mappings_outputs\n",
    "\n",
    "\n",
    "# maximum length of output sequences\n",
    "max_len_outputs = max(len(i) for i in target_sequences)\n",
    "print(\"Maximum length of output sequences:\",max_len_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fd83395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3266"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words_output = len(word_to_idx_outputs) + 1\n",
    "num_words_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30f763",
   "metadata": {},
   "source": [
    "### Pad each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a42825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input shape is (3061, 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1326],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding for encoder inputs\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_inputs)\n",
    "print(\"Encoder input shape is\", encoder_inputs.shape)\n",
    "encoder_inputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dedbb97",
   "metadata": {},
   "source": [
    "#### Upon seeing the encoder state/the last word of the input sequence, the decoder produces the output immediately rather than having to go through a bunch of zeros first, hence post padding for decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "903d6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input shape is (3061, 26)\n",
      "[   2 1538    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "Decoder target shape is (3061, 26)\n",
      "[1538    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# padding for decoder inputs and targets\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_outputs, padding='post')\n",
    "print(\"Decoder input shape is\", decoder_inputs.shape)\n",
    "print(decoder_inputs[0])\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_outputs, padding='post')\n",
    "print(\"Decoder target shape is\", decoder_targets.shape)\n",
    "print(decoder_targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc1b6c7",
   "metadata": {},
   "source": [
    "### Storing pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c10a77f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "print(\"Filling pre-trained embeddings...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ac5a3e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/saylinarkhede/Jupyter/github_/neural_machine_translation/glove'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() + '/glove'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e64f000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "word_to_vec = {}\n",
    "with open(os.path.join(os.getcwd() + '/glove/glove.6B.%sd.txt' %EMBEDDING_DIM)) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word_to_vec[word] = vec\n",
    "        \n",
    "print('Found %s word vectors' %len(word_to_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27eb0736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length is 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "        -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "         0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "        -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "         0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "        -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "         0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "         0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "        -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "        -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "        -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "        -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "        -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "        -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "        -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "         0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "        -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32),\n",
       " ',': array([-0.10767  ,  0.11053  ,  0.59812  , -0.54361  ,  0.67396  ,\n",
       "         0.10663  ,  0.038867 ,  0.35481  ,  0.06351  , -0.094189 ,\n",
       "         0.15786  , -0.81665  ,  0.14172  ,  0.21939  ,  0.58505  ,\n",
       "        -0.52158  ,  0.22783  , -0.16642  , -0.68228  ,  0.3587   ,\n",
       "         0.42568  ,  0.19021  ,  0.91963  ,  0.57555  ,  0.46185  ,\n",
       "         0.42363  , -0.095399 , -0.42749  , -0.16567  , -0.056842 ,\n",
       "        -0.29595  ,  0.26037  , -0.26606  , -0.070404 , -0.27662  ,\n",
       "         0.15821  ,  0.69825  ,  0.43081  ,  0.27952  , -0.45437  ,\n",
       "        -0.33801  , -0.58184  ,  0.22364  , -0.5778   , -0.26862  ,\n",
       "        -0.20425  ,  0.56394  , -0.58524  , -0.14365  , -0.64218  ,\n",
       "         0.0054697, -0.35248  ,  0.16162  ,  1.1796   , -0.47674  ,\n",
       "        -2.7553   , -0.1321   , -0.047729 ,  1.0655   ,  1.1034   ,\n",
       "        -0.2208   ,  0.18669  ,  0.13177  ,  0.15117  ,  0.7131   ,\n",
       "        -0.35215  ,  0.91348  ,  0.61783  ,  0.70992  ,  0.23955  ,\n",
       "        -0.14571  , -0.37859  , -0.045959 , -0.47368  ,  0.2385   ,\n",
       "         0.20536  , -0.18996  ,  0.32507  , -1.1112   , -0.36341  ,\n",
       "         0.98679  , -0.084776 , -0.54008  ,  0.11726  , -1.0194   ,\n",
       "        -0.24424  ,  0.12771  ,  0.013884 ,  0.080374 , -0.35414  ,\n",
       "         0.34951  , -0.7226   ,  0.37549  ,  0.4441   , -0.99059  ,\n",
       "         0.61214  , -0.35111  , -0.83155  ,  0.45293  ,  0.082577 ],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "first_2_vecs = dict(islice(word_to_vec.items(), 2))\n",
    "print('Vector length is', len(list(first_2_vecs.items())[0][1]))\n",
    "first_2_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d742dc6",
   "metadata": {},
   "source": [
    "### Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25dc85eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5dddb99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_to_idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros(shape=(num_words, EMBEDDING_DIM))\n",
    "embedding_matrix.shape\n",
    "\n",
    "for word, i in word_to_idx_inputs.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word_to_vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "756de2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2464, 100)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c624518",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "530a19d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                              EMBEDDING_DIM,\n",
    "                              weights=[embedding_matrix],\n",
    "                              input_length = max_len_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52528bb1",
   "metadata": {},
   "source": [
    "### One-hot encoded target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ec6ac",
   "metadata": {},
   "source": [
    "#### Creating targets and since the targets are sequences, I can not use sparse categorical cross entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "397b4037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3061\n",
      "26\n",
      "3266\n",
      "(3061, 26)\n"
     ]
    }
   ],
   "source": [
    "print(len(input_texts))\n",
    "print(max_len_outputs)\n",
    "print(num_words_output)\n",
    "print(decoder_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8b7b88f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot = np.zeros((len(input_texts), max_len_outputs, num_words_output), dtype='float32')\n",
    "decoder_targets_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bab6b2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3061, 26, 3266)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, d in enumerate(decoder_targets):\n",
    "    for t, word in enumerate(d):\n",
    "        if word != 0:\n",
    "            decoder_targets_one_hot[i, t, word] = 1\n",
    "            \n",
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2888eae1",
   "metadata": {},
   "source": [
    "### Building a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c2ef790",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LATENT_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3747cd76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Input, Dense, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9774a438",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                              EMBEDDING_DIM,\n",
    "                              weights=[embedding_matrix],\n",
    "                              input_length = max_len_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32e0c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up encoder\n",
    "encoder_inputs_placeholder = Input(shape=(max_len_inputs,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "\n",
    "encoder = LSTM(LATENT_DIM, return_state=True)\n",
    "\n",
    "encoder_outputs, h, c = encoder(x)\n",
    "encoder_states = [h, c]\n",
    "# print(encoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2c0970b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up decoder\n",
    "decoder_inputs_placeholder = Input(shape = (max_len_outputs,))\n",
    "decoder_embedding = Embedding(num_words_output, LATENT_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences = True, return_state = True)\n",
    "\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs_x,\n",
    "                                     initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73831145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final dense layer for prediction\n",
    "decoder_dense = Dense(num_words_output, activation = 'softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a1f2964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "from keras.models import Model\n",
    "\n",
    "model = Model([encoder_inputs_placeholder, decoder_inputs_placeholder], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27df1de5",
   "metadata": {},
   "source": [
    "### Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6325981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e1432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "39/39 [==============================] - 14s 347ms/step - loss: 1.5986 - accuracy: 0.0425 - val_loss: 2.7009 - val_accuracy: 0.0412\n",
      "Epoch 2/40\n",
      "39/39 [==============================] - 13s 339ms/step - loss: 1.5897 - accuracy: 0.0427 - val_loss: 2.6972 - val_accuracy: 0.0415\n",
      "Epoch 3/40\n",
      "39/39 [==============================] - 14s 367ms/step - loss: 1.5814 - accuracy: 0.0429 - val_loss: 2.6917 - val_accuracy: 0.0415\n",
      "Epoch 4/40\n",
      "39/39 [==============================] - 16s 417ms/step - loss: 1.5718 - accuracy: 0.0430 - val_loss: 2.6824 - val_accuracy: 0.0416\n",
      "Epoch 5/40\n",
      "39/39 [==============================] - 15s 384ms/step - loss: 1.5588 - accuracy: 0.0430 - val_loss: 2.6735 - val_accuracy: 0.0428\n",
      "Epoch 6/40\n",
      "39/39 [==============================] - 14s 353ms/step - loss: 1.5469 - accuracy: 0.0430 - val_loss: 2.6654 - val_accuracy: 0.0419\n",
      "Epoch 7/40\n",
      "39/39 [==============================] - 17s 427ms/step - loss: 1.5433 - accuracy: 0.0424 - val_loss: 2.6656 - val_accuracy: 0.0428\n",
      "Epoch 8/40\n",
      "39/39 [==============================] - 14s 366ms/step - loss: 1.5282 - accuracy: 0.0436 - val_loss: 2.6579 - val_accuracy: 0.0426\n",
      "Epoch 9/40\n",
      "39/39 [==============================] - 14s 350ms/step - loss: 1.5192 - accuracy: 0.0440 - val_loss: 2.6551 - val_accuracy: 0.0439\n",
      "Epoch 10/40\n",
      "39/39 [==============================] - 13s 342ms/step - loss: 1.5111 - accuracy: 0.0449 - val_loss: 2.6497 - val_accuracy: 0.0454\n",
      "Epoch 11/40\n",
      "39/39 [==============================] - 13s 338ms/step - loss: 1.5037 - accuracy: 0.0452 - val_loss: 2.6445 - val_accuracy: 0.0457\n",
      "Epoch 12/40\n",
      "39/39 [==============================] - 13s 340ms/step - loss: 1.4950 - accuracy: 0.0455 - val_loss: 2.6416 - val_accuracy: 0.0457\n",
      "Epoch 13/40\n",
      "39/39 [==============================] - 13s 335ms/step - loss: 1.4875 - accuracy: 0.0459 - val_loss: 2.6361 - val_accuracy: 0.0472\n",
      "Epoch 14/40\n",
      "39/39 [==============================] - 13s 330ms/step - loss: 1.4796 - accuracy: 0.0463 - val_loss: 2.6370 - val_accuracy: 0.0466\n",
      "Epoch 15/40\n",
      "39/39 [==============================] - 13s 332ms/step - loss: 1.4732 - accuracy: 0.0466 - val_loss: 2.6282 - val_accuracy: 0.0475\n",
      "Epoch 16/40\n",
      "39/39 [==============================] - 13s 344ms/step - loss: 1.4646 - accuracy: 0.0469 - val_loss: 2.6325 - val_accuracy: 0.0482\n",
      "Epoch 17/40\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 1.4583 - accuracy: 0.0475 - val_loss: 2.6229 - val_accuracy: 0.0484\n",
      "Epoch 18/40\n",
      "39/39 [==============================] - 15s 378ms/step - loss: 1.4509 - accuracy: 0.0477 - val_loss: 2.6230 - val_accuracy: 0.0478\n",
      "Epoch 19/40\n",
      "39/39 [==============================] - 14s 348ms/step - loss: 1.4437 - accuracy: 0.0480 - val_loss: 2.6195 - val_accuracy: 0.0475\n",
      "Epoch 20/40\n",
      "39/39 [==============================] - 14s 352ms/step - loss: 1.4364 - accuracy: 0.0489 - val_loss: 2.6192 - val_accuracy: 0.0488\n",
      "Epoch 21/40\n",
      "39/39 [==============================] - 14s 370ms/step - loss: 1.4293 - accuracy: 0.0491 - val_loss: 2.6153 - val_accuracy: 0.0489\n",
      "Epoch 22/40\n",
      "39/39 [==============================] - 13s 342ms/step - loss: 1.4221 - accuracy: 0.0501 - val_loss: 2.6185 - val_accuracy: 0.0480\n",
      "Epoch 23/40\n",
      "39/39 [==============================] - 14s 367ms/step - loss: 1.4157 - accuracy: 0.0506 - val_loss: 2.6143 - val_accuracy: 0.0482\n",
      "Epoch 24/40\n",
      "39/39 [==============================] - 18s 460ms/step - loss: 1.4089 - accuracy: 0.0509 - val_loss: 2.6190 - val_accuracy: 0.0493\n",
      "Epoch 25/40\n",
      "39/39 [==============================] - 19s 500ms/step - loss: 1.4020 - accuracy: 0.0517 - val_loss: 2.6114 - val_accuracy: 0.0493\n",
      "Epoch 26/40\n",
      "39/39 [==============================] - 20s 509ms/step - loss: 1.3951 - accuracy: 0.0523 - val_loss: 2.6099 - val_accuracy: 0.0499\n",
      "Epoch 27/40\n",
      "39/39 [==============================] - 14s 361ms/step - loss: 1.3878 - accuracy: 0.0527 - val_loss: 2.6090 - val_accuracy: 0.0503\n",
      "Epoch 28/40\n",
      "39/39 [==============================] - 15s 391ms/step - loss: 1.3814 - accuracy: 0.0539 - val_loss: 2.6069 - val_accuracy: 0.0508\n",
      "Epoch 29/40\n",
      "39/39 [==============================] - 15s 396ms/step - loss: 1.3739 - accuracy: 0.0547 - val_loss: 2.6037 - val_accuracy: 0.0513\n",
      "Epoch 30/40\n",
      "39/39 [==============================] - 16s 424ms/step - loss: 1.3665 - accuracy: 0.0553 - val_loss: 2.6044 - val_accuracy: 0.0518\n",
      "Epoch 31/40\n",
      "39/39 [==============================] - 15s 383ms/step - loss: 1.3596 - accuracy: 0.0564 - val_loss: 2.6000 - val_accuracy: 0.0528\n",
      "Epoch 32/40\n",
      "39/39 [==============================] - 14s 370ms/step - loss: 1.3532 - accuracy: 0.0576 - val_loss: 2.6002 - val_accuracy: 0.0526\n",
      "Epoch 33/40\n",
      "39/39 [==============================] - 20s 516ms/step - loss: 1.3465 - accuracy: 0.0587 - val_loss: 2.6005 - val_accuracy: 0.0534\n",
      "Epoch 34/40\n",
      "39/39 [==============================] - 36s 945ms/step - loss: 1.3398 - accuracy: 0.0592 - val_loss: 2.5976 - val_accuracy: 0.0539\n",
      "Epoch 35/40\n",
      "39/39 [==============================] - 20s 498ms/step - loss: 1.3325 - accuracy: 0.0600 - val_loss: 2.5941 - val_accuracy: 0.0533\n",
      "Epoch 36/40\n",
      "39/39 [==============================] - 20s 512ms/step - loss: 1.3260 - accuracy: 0.0603 - val_loss: 2.5934 - val_accuracy: 0.0552\n",
      "Epoch 37/40\n",
      "39/39 [==============================] - 18s 454ms/step - loss: 1.3197 - accuracy: 0.0609 - val_loss: 2.5977 - val_accuracy: 0.0539\n",
      "Epoch 38/40\n",
      "39/39 [==============================] - 21s 535ms/step - loss: 1.3137 - accuracy: 0.0625 - val_loss: 2.5929 - val_accuracy: 0.0532\n",
      "Epoch 39/40\n",
      "33/39 [========================>.....] - ETA: 1s - loss: 1.3054 - accuracy: 0.0638"
     ]
    }
   ],
   "source": [
    "r = model.fit([encoder_inputs, decoder_inputs], \n",
    "          decoder_targets_one_hot,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          epochs = 40, \n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label = 'loss')\n",
    "plt.plot(r.history['val_loss'], label = 'val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbcfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['acc'], label = 'accuracy')\n",
    "plt.plot(r.history['val_acc'], label = 'val_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36291d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
