{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0c3ee0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5379199",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "705e6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10000\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "target_texts_input = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0b03f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0\n",
    "\n",
    "with open(\"../neural_machine_translation/data/hin.txt\") as f:\n",
    "    for line in f:\n",
    "        \n",
    "        # keeping only limited number of samples\n",
    "        t += 1\n",
    "        if t > NUM_SAMPLES:\n",
    "            break\n",
    "            \n",
    "        if '\\t' not in line:\n",
    "            continue\n",
    "            \n",
    "        input_text, translation, *rest = line.rstrip().split('\\t')\n",
    "        \n",
    "        target_text = translation + ' <eos>'\n",
    "        target_text_input = '<sos> ' + translation\n",
    "        \n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "        target_texts_input.append(target_text_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b132a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> वाह!',\n",
       " '<sos> झुको!',\n",
       " '<sos> बतख़!',\n",
       " '<sos> बचाओ!',\n",
       " '<sos> उछलो.',\n",
       " '<sos> कूदो.',\n",
       " '<sos> छलांग.',\n",
       " '<sos> नमस्ते।',\n",
       " '<sos> नमस्कार।',\n",
       " '<sos> वाह-वाह!',\n",
       " '<sos> चियर्स!',\n",
       " '<sos> सांस छोड़।',\n",
       " '<sos> सांस छोड़ो।',\n",
       " '<sos> समझे कि नहीं?',\n",
       " '<sos> मैं ठीक हूँ।']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts_input[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6b092",
   "metadata": {},
   "source": [
    "### Tokenizers and word to index mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828b992",
   "metadata": {},
   "source": [
    "#### I have two languages to deal with, hence need two different tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b3e21b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wow!', 'Duck!', 'Duck!', 'Help!', 'Jump.']\n"
     ]
    }
   ],
   "source": [
    "print(input_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0a12a2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from itertools import islice\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "MAX_SEQ_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7df03c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1326], [949], [949], [83], [582], [582], [582]]\n",
      "Found 2463 unique input tokens\n",
      "Maximum length of input sequences: 22\n"
     ]
    }
   ],
   "source": [
    "# tokenizer for inputs\n",
    "tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "print(input_sequences[:7])\n",
    "\n",
    "\n",
    "# word_to_index mapping \n",
    "word_to_idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens' % len(word_to_idx_inputs))\n",
    "first_10_mapping_inputs = dict(islice(word_to_idx_inputs.items(), 10))\n",
    "first_10_mapping_inputs\n",
    "\n",
    "\n",
    "# maximum length of input sequences\n",
    "max_len_inputs = max(len(i) for i in input_sequences)\n",
    "print(\"Maximum length of input sequences:\",max_len_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ffc88fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1538, 1], [1539, 1], [1540, 1], [1541, 1], [1542, 1]]\n",
      "[[2, 1538], [2, 1539], [2, 1540], [2, 1541], [2, 1542]]\n",
      "Found 3265 unique output tokens\n",
      "Maximum length of output sequences: 26\n"
     ]
    }
   ],
   "source": [
    "# tokenizer for outputs\n",
    "tokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_input)\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_input)\n",
    "print(target_sequences[:5])\n",
    "print(target_sequences_inputs[:5])\n",
    "\n",
    "\n",
    "# word to index mapping\n",
    "word_to_idx_outputs = tokenizer_outputs.word_index\n",
    "print(\"Found %s unique output tokens\" % len(word_to_idx_outputs))\n",
    "first_10_mappings_outputs = dict(islice(word_to_idx_outputs.items(), 10))\n",
    "first_10_mappings_outputs\n",
    "\n",
    "\n",
    "# maximum length of output sequences\n",
    "max_len_outputs = max(len(i) for i in target_sequences)\n",
    "print(\"Maximum length of output sequences:\",max_len_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a20a44da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3266"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words_output = len(word_to_idx_outputs) + 1\n",
    "num_words_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9650ba0b",
   "metadata": {},
   "source": [
    "### Pad each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "52c5e176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input shape is (3061, 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 1326],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding for encoder inputs\n",
    "encoder_inputs = pad_sequences(input_sequences, maxlen=max_len_inputs)\n",
    "print(\"Encoder input shape is\", encoder_inputs.shape)\n",
    "encoder_inputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58dca4f",
   "metadata": {},
   "source": [
    "#### Upon seeing the encoder state/the last word of the input sequence, the decoder produces the output immediately rather than having to go through a bunch of zeros first, hence post padding for decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18c202da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder input shape is (3061, 26)\n",
      "[   2 1538    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "Decoder target shape is (3061, 26)\n",
      "[1538    1    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# padding for decoder inputs and targets\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_outputs, padding='post')\n",
    "print(\"Decoder input shape is\", decoder_inputs.shape)\n",
    "print(decoder_inputs[0])\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen=max_len_outputs, padding='post')\n",
    "print(\"Decoder target shape is\", decoder_targets.shape)\n",
    "print(decoder_targets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7a4a68",
   "metadata": {},
   "source": [
    "### Storing pre-trained vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b3863597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "\n",
    "print(\"Filling pre-trained embeddings...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "63d11026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/saylinarkhede/Jupyter/github_/neural_machine_translation'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd() + '/glove'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d719b9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors\n"
     ]
    }
   ],
   "source": [
    "word_to_vec = {}\n",
    "with open(os.path.join(os.getcwd() + '/glove/glove.6B.%sd.txt' %EMBEDDING_DIM)) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        word_to_vec[word] = vec\n",
    "        \n",
    "print('Found %s word vectors' %len(word_to_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e486c51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length is 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'the': array([-0.038194, -0.24487 ,  0.72812 , -0.39961 ,  0.083172,  0.043953,\n",
       "        -0.39141 ,  0.3344  , -0.57545 ,  0.087459,  0.28787 , -0.06731 ,\n",
       "         0.30906 , -0.26384 , -0.13231 , -0.20757 ,  0.33395 , -0.33848 ,\n",
       "        -0.31743 , -0.48336 ,  0.1464  , -0.37304 ,  0.34577 ,  0.052041,\n",
       "         0.44946 , -0.46971 ,  0.02628 , -0.54155 , -0.15518 , -0.14107 ,\n",
       "        -0.039722,  0.28277 ,  0.14393 ,  0.23464 , -0.31021 ,  0.086173,\n",
       "         0.20397 ,  0.52624 ,  0.17164 , -0.082378, -0.71787 , -0.41531 ,\n",
       "         0.20335 , -0.12763 ,  0.41367 ,  0.55187 ,  0.57908 , -0.33477 ,\n",
       "        -0.36559 , -0.54857 , -0.062892,  0.26584 ,  0.30205 ,  0.99775 ,\n",
       "        -0.80481 , -3.0243  ,  0.01254 , -0.36942 ,  2.2167  ,  0.72201 ,\n",
       "        -0.24978 ,  0.92136 ,  0.034514,  0.46745 ,  1.1079  , -0.19358 ,\n",
       "        -0.074575,  0.23353 , -0.052062, -0.22044 ,  0.057162, -0.15806 ,\n",
       "        -0.30798 , -0.41625 ,  0.37972 ,  0.15006 , -0.53212 , -0.2055  ,\n",
       "        -1.2526  ,  0.071624,  0.70565 ,  0.49744 , -0.42063 ,  0.26148 ,\n",
       "        -1.538   , -0.30223 , -0.073438, -0.28312 ,  0.37104 , -0.25217 ,\n",
       "         0.016215, -0.017099, -0.38984 ,  0.87424 , -0.72569 , -0.51058 ,\n",
       "        -0.52028 , -0.1459  ,  0.8278  ,  0.27062 ], dtype=float32),\n",
       " ',': array([-0.10767  ,  0.11053  ,  0.59812  , -0.54361  ,  0.67396  ,\n",
       "         0.10663  ,  0.038867 ,  0.35481  ,  0.06351  , -0.094189 ,\n",
       "         0.15786  , -0.81665  ,  0.14172  ,  0.21939  ,  0.58505  ,\n",
       "        -0.52158  ,  0.22783  , -0.16642  , -0.68228  ,  0.3587   ,\n",
       "         0.42568  ,  0.19021  ,  0.91963  ,  0.57555  ,  0.46185  ,\n",
       "         0.42363  , -0.095399 , -0.42749  , -0.16567  , -0.056842 ,\n",
       "        -0.29595  ,  0.26037  , -0.26606  , -0.070404 , -0.27662  ,\n",
       "         0.15821  ,  0.69825  ,  0.43081  ,  0.27952  , -0.45437  ,\n",
       "        -0.33801  , -0.58184  ,  0.22364  , -0.5778   , -0.26862  ,\n",
       "        -0.20425  ,  0.56394  , -0.58524  , -0.14365  , -0.64218  ,\n",
       "         0.0054697, -0.35248  ,  0.16162  ,  1.1796   , -0.47674  ,\n",
       "        -2.7553   , -0.1321   , -0.047729 ,  1.0655   ,  1.1034   ,\n",
       "        -0.2208   ,  0.18669  ,  0.13177  ,  0.15117  ,  0.7131   ,\n",
       "        -0.35215  ,  0.91348  ,  0.61783  ,  0.70992  ,  0.23955  ,\n",
       "        -0.14571  , -0.37859  , -0.045959 , -0.47368  ,  0.2385   ,\n",
       "         0.20536  , -0.18996  ,  0.32507  , -1.1112   , -0.36341  ,\n",
       "         0.98679  , -0.084776 , -0.54008  ,  0.11726  , -1.0194   ,\n",
       "        -0.24424  ,  0.12771  ,  0.013884 ,  0.080374 , -0.35414  ,\n",
       "         0.34951  , -0.7226   ,  0.37549  ,  0.4441   , -0.99059  ,\n",
       "         0.61214  , -0.35111  , -0.83155  ,  0.45293  ,  0.082577 ],\n",
       "       dtype=float32)}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "first_2_vecs = dict(islice(word_to_vec.items(), 2))\n",
    "print('Vector length is', len(list(first_10_vecs.items())[0][1]))\n",
    "first_2_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137c6e8",
   "metadata": {},
   "source": [
    "### Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "eb07bd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2767ae26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_to_idx_inputs) + 1)\n",
    "embedding_matrix = np.zeros(shape=(num_words, EMBEDDING_DIM))\n",
    "embedding_matrix.shape\n",
    "\n",
    "for word, i in word_to_idx_inputs.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word_to_vec.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0da6a0b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2464, 100)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2f382",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1ed05e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words,\n",
    "                              EMBEDDING_DIM,\n",
    "                              weights=\"embedding_matrix\",\n",
    "                              input_length = max_len_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7c483",
   "metadata": {},
   "source": [
    "### One-hot encoded target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94d822a",
   "metadata": {},
   "source": [
    "#### Creating targets and since the targets are sequences, I can not use sparse categorical cross entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "40ae0891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3061\n",
      "26\n",
      "3266\n",
      "(3061, 26)\n"
     ]
    }
   ],
   "source": [
    "print(len(input_texts))\n",
    "print(max_len_outputs)\n",
    "print(num_words_output)\n",
    "print(decoder_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0bf83406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot = np.zeros((len(input_texts), max_len_outputs, num_words_output), dtype='float32')\n",
    "decoder_targets_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3b2efe97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3061, 26, 3266)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, d in enumerate(decoder_targets):\n",
    "    for t, word in enumerate(d):\n",
    "        if word != 0:\n",
    "            decoder_targets_one_hot[i, t, word] = 1\n",
    "            \n",
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f51b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213bb37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
